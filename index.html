<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog: GitLab Experiments</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            padding-top: 50px;
            background-color: #f9f9f9;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: auto;
            background: #fff;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        h1, h2, h3 {
            color: #333;
        }
        p {
            margin: 10px 0;
        }
        pre {
            background-color: #eee;
            border-radius: 5px;
            padding: 10px;
            overflow-x: auto;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
            color: #d63384;
        }
        .code-block {
            background-color: #f4f4f4;
            border-left: 3px solid #d63384;
            margin: 20px 0;
            padding: 10px;
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Blog Post: GitLab Experiments</h1>
    <p>by Adam Knapp </p>
    <p>Posted on March 25, 2024, for Machine Learning in Production (Carnegie Mellon University)</p>

    <h2>Outline</h2>
    <p>Here's a quick overview of what is covered:</p>
    <ul>
        <li><a href="#gitlab-background">GitLab Background</a></li>
        <li><a href="#gitlab-ml-ops">GitLab ML Ops</a></li>
        <li>
            <a href="#gitlab-experiments">GitLab Experiments</a>
            <ul>
                <li><a href="#gitlab-experiments-overview">Overview</a></li>
                <li><a href="#gitlab-experiments-setup">Setup</a></li>
                <li><a href="#gitlab-experiments-example">Example</a></li>
            </ul>
        </li>
        <li><a href="#conclusion">Conclusion</a></li>
    </ul>

    <h2 id="gitlab-background">GitLab Background</h2>
    <p><a href="https://about.gitlab.com/">GitLab</a>, founded in 2014, quickly emerged as a frontrunner in the DevOps 
        toolchain, offering a single application for the entire software development lifecycle. From project planning and 
        source code management to CI/CD, monitoring, and security, GitLab has provided comprehensive solutions that 
        streamline productivity and foster a collaborative environment for software development teams. Its ability to 
        support both the development and operations side of projects makes it particularly appealing for managing machine 
        learning (ML) projects, which inherently require cross-disciplinary collaboration between software engineers and 
        data scientists.</p>

    <h2 id="gitlab-ml-ops">GitLab ML Ops</h2>
    <p>MLOps, a compound of machine learning and operations, aims to unify ML system development and ML system deployment. 
        The goal of MLOPs is to shorten the lifecycle of deploying machine learning models and provide continuous feedback between models 
        and development to ensure models are valuable and efficient. GitLab naturally facilitates MLOps with its DevOps toolchain (ie. CI/CD 
        automated testing, monitoring, ect.) but they saw the need to include some MLOps specific tools to reduce the gap between data scientist 
        and software developers.</p>

    <figure>
    <img src="Images/mlops-cycle.png" alt="Overview of MLOps" style="max-width:100%; height:auto;">
    <figcaption>Visual representation of a MLOps Diagram (https://polyaxon.com/)</figcaption>
    </figure>
    

    <p>One major hurdle for MLOps is the messiness of model exploration. ML developers typically use jupyter notebooks and don't structure the notebook in a standardized way
        making each notebook hard for colaberation. To combat this phenomina, Model registry tools like MLflow, W&B, ect. were made to help track 
        model production and help facilitate the effective sharing of models. These tools allow developers to log aspects of their models to ensure reproducability for organization
     </p>
    
     <p> GitLab added the benifits of a model registry by incorperating MLFlow into their interface. First, the MFflow client 
    which would normally be hosted on a seperate server is hosted on the GitLab endpoint allowing all team members visibility into 
    ML model production without the overhead of running a seperate endpoint. Second, GitLab nests it's experiements with 
    ML Flows allowing ML developers to log and store information about significant model's trained to assist in sharing within the team. Third,
    GitLab included a Model Registry section which powers versioning models for deployment. The model registry can take a model experiment and 
    move it into a condolidated location where operators have a the latest production ready models available with their artifiacts.
    Finally using exisiting GitLab DEV tools, the deployment of models can be automated into new or existing pipelines</p>


    <h2 id="gitlab-experiments">GitLab Experiments</h2>

    <h3 id="gitlab-experiments-overview">Overview</h3>

    <p>Experimentation is at the heart of machine learning. With <a href="https://docs.gitlab.com/ee/user/project/ml/experiment_tracking/#machine-learning-model-experiments">GitLab experiments</a>,   
        ML developers can log their models 
        in GitLab with the information needed to share their findings and make the model run reproducable.It is commonly perceived 
    that the Code, Data, and Enviornment are needed to be tracked with a model to make it reproducable. We'll highlight these aspects
    in our example</p>
        

    <p>GitLab experiments does not force you to log your model in a way to ensure it is reporducable, but in my example below 
        I will highlight some basic logs. Understand that every team is different and you will need to cator how you use experiments 
        to your team.  
    </p>

    <h3 id="gitlab-experiments-setup">Setup</h3>

    <p>This feature is still in Beta testing and has very limited documentaion. I will provide a step by step tutorial on 
        how to implement so that you don't have the same issues I did. To complete this you should already have a GitLab account
        with at least one project.
    </p>
    

    <h4>Set Up GitLab API Key</h4>

    <ul>
        <li>Project Tokens 
            <figure>
                <img src="Images/AccessTokens.png" alt="Tokens" style="max-width:100%; height:auto;">
                <figcaption>In your project, Navigate to 'SETTINGS' -> 'ACCESS TOKENS'</figcaption>
                </figure>
        </li>
        <li>Add Token 
            <figure>
                <img src="Images/AddNewToken.png" alt="Screen shot on: How to add a new token" style="max-width:100%; height:auto;">
                <figcaption>Navigate to 'ADD NEW TOKEN'</figcaption>
                </figure>
        </li>
        <li>Token Configuration
            <figure>
                <img src="Images/AddaProjectToken.png" alt="Screen shot on: How to configure a project Token" style="max-width:100%; height:auto;">
                <figcaption>Name you token, Set exporation date, Select 'DEVELOPER' role (Experiments needs minimum of Developer role), Check "API" for Access Scope (Required Scope)</figcaption>
                </figure> 
        </li>
        <li>Save Token 
            <figure>
                <img src="Images/SaveProjectToken.png" alt="Screen shot on: Where to save token" style="max-width:100%; height:auto;">
                <figcaption>Copy and save this token for later use</figcaption>
                </figure> 
        </li>
    </ul>

    <h4>Configuration</h4>

    <ul>
        <li>Obtain Project ID
            <ul>
                <li>General Settings
                    <figure>
                        <img src="Images/GeneralSettings.png" alt="Screen shot on: How to Navigate to general settings" style="max-width:100%; height:auto;">
                        <figcaption>From a project, navigate to SETTINGS -> GENERAL</figcaption>
                        </figure> 
                </li>
                <li>Project ID
                    <figure>
                        <img src="Images/ProjectID.png" alt="Screen shot on: Project ID" style="max-width:100%; height:auto;">
                        <figcaption>Copy and save your project ID for later use</figcaption>
                        </figure> 
                </li>
        </ul></li>
        <li>Install ML Flow 
            <div class="code-block">
                <pre><code>
                    pip install mlflow == 2.11.2 
                </div>
        </li>
        <li>Set endpoint of MLFlow to Gitlab
            <div class="code-block">
                <pre><code>
                    os.environ["MLFLOW_TRACKING_TOKEN"]='[Saved API Token]'
                    os.environ["MLFLOW_TRACKING_URI"]='http://[your gitlab instance]/api/v4/projects/[your project id]/ml/mlflow'</code></pre>
            </div>
            <p>Note: your gitlab instance relates to the first part of the url. Ex. for "https://gitlab.com/group-name/project-name" 
            the gitlab instance is "gitlab.com"</p>
        
        </li>
        <li>Use ML Flow library to log experiments in .py or .ipynb files
            <div class="code-block">
                <pre><code>
                #Example usage

                import mlflow

                #Set the experiment name
                mlflow.set_experiment(experiment_name=f"Experiment")  

                #Set the run in the experiment name
                mlflow.start_run(run_name=f"Run")
                
                #Log model parameters for model
                mlflow.log_param("model parameter", 0)

                #Log metric for model
                mlflow.log_metric("Model Metric", 0)

                #Log an artifiact to be saved with the run
                mlflow.log_artifact('file location')

                #end the logging 
                mlflow.end_run()
                
                </code></pre>
            </div>
        
        </li>
    </ul>



    <h3 id="gitlab-experiments-example">Movie Recommendations Example</h3>

    <p>In this example we'll explore model experiments being used in a movie recommendation model
    </p>

    <div class="code-block">
        <pre><code>
            #imports
            import os
            import sys
            import json
            import subprocess
            import mlflow
            from surprise import Dataset, Reader, SVD, accuracy
            from surprise.model_selection import train_test_split, GridSearchCV
            import pickle
</div>

    <div class="code-block">
        <pre><code>
            #import data
            data_path = os.path.join("..", "..", "data", "kafka_log_(2024-02-11T16_2024-02-11T16).csv")
            
            xx, user_rating_data_df, xxx = utilities.process_csv(data_path)
        
            os.environ["MLFLOW_TRACKING_TOKEN"]='[Saved API Token]'
            os.environ["MLFLOW_TRACKING_URI"]='https://gitlab.com/api/v4/projects/56020467/ml/mlflow'</code></pre>
        </div>

    <div class="code-block">
        <pre><code>
            os.environ["MLFLOW_TRACKING_TOKEN"]='[Saved API Token]'
            os.environ["MLFLOW_TRACKING_URI"]='http://[your gitlab instance]/api/v4/projects/[your project id]/ml/mlflow'</code></pre>
    </div>

    <div class="code-block">
        <pre><code>
            
        def train_and_evaluate_model(user_rating_data_df):
            reader = Reader(rating_scale=(1, 5))
            data = Dataset.load_from_df(user_rating_data_df[['UserID', 'Title', 'Rating']], reader)

            param_grid = {
                'n_factors': [50, 100, 150],
                'n_epochs': [20, 30],
                'lr_all': [0.005, 0.010],
                'reg_all': [0.02, 0.05]
            }

            gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=-1)
            gs.fit(data)

            best_model_params = gs.best_params['rmse']
            print(f"Best model parameters: {best_model_params}")

            # Splitting the dataset into training and testing sets
            trainset, testset = train_test_split(data, test_size=0.25)

            # Initialize the best model with the best parameters
            best_model = SVD(**best_model_params)

            # Directly fit the best model to the training set
            best_model.fit(trainset)

            # Test the best model on the test set
            predictions = best_model.test(testset)

            # Calculate and print the performance metrics
            rmse = accuracy.rmse(predictions, verbose=True)
            mae = accuracy.mae(predictions, verbose=True)

            print(f"Test Set RMSE: {rmse}")
            print(f"Test Set MAE: {mae}")


            ### Log best model ###

            #Set up the experiment and the run in that experiment
            mlflow.set_experiment(experiment_name=f'SVD_v0') #named based on the type of model
            mlflow.start_run(run_name=f"Candidate {2}")

            # Log the best hyperparameters
            for param, value in best_model_params.items():
                mlflow.log_param(f"best_{param}", value)

            # Log performance of model 
            mlflow.log_metric(f"RMSE", rmse)
            mlflow.log_metric(f"MAE", mae)

            # Log dataset path 
            csv_file_path = "data/kafka_log_(2024-02-11T16_2024-02-11T16).csv"
            mlflow.log_param("dataset_path", csv_file_path)

            # Log Python version
            mlflow.log_param("python_version", sys.version)

            # Log requirements.txt and make a requirements.txt if not there
            requirements_file = "requirements.txt"
            if not os.path.exists(requirements_file):
                print("requirements.txt not found, generating...")
                requirements = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).decode('utf-8')
                with open(requirements_file, "w") as f:
                    f.write(requirements)
            mlflow.log_artifact(requirements_file)

            # Serialize and log the best model
            model_filename = "best_SVD_model.pkl"
            with open(model_filename, "wb") as f:
                pickle.dump(best_model, f)
            mlflow.log_artifact(model_filename)

            #end the logging 
            mlflow.end_run()

            return best_model

        train_and_evaluate_model(user_rating_data_df)   
        </div>

        <p> After running the above code the SVD experiment will populate in Gitlab. The following screenshots highlight 
            viewing a logged model in Gitlab Experiments.
        </p>

        <figure>
            <img src="Images/ModelExperiment.png" style="max-width:100%; height:auto;">
            <figcaption>Navigate to Model Experiments by Selecting ANALYIS -> EXPERIMENTS</figcaption>
            </figure>

        <figure>
            <img src="Images/Experiments.png" style="max-width:100%; height:auto;">
            <figcaption>Select Experiment you would like to explore</figcaption>
            </figure>
    

        <figure>
            <img src="Images/Run1.png"  style="max-width:100%; height:auto;">
            </figure>

            <figure>
                <img src="Images/Run2.png" style="max-width:100%; height:auto;">
                </figure>
            <figure>
                <img src="Images/Artifacts.png" style="max-width:100%; height:auto;">
                <figcaption>After selecting ARTIFACTS, all logged files can be found and downloaded</figcaption>
                </figure>

    <h3 id="conclusion">Conclusion</h3>       

    <p>MLOps, a compound of machine learning and operations, aims to unify ML system development and ML system deployment. 
        The goal of MLOPs is to shorten the lifecycle of deploying machine learning models and provide continuous feedback between models 
        and development to ensure models are valuable and efficient. GitLab naturally facilitates MLOps with its DevOps toolchain (ie. CI/CD 
        automated testing, monitoring, ect.) but they saw the need to include some MLOps specific tools to reduce the gap between data scientist 
        and software developers.</p>
 

</body>
</html>
